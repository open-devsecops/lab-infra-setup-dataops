trigger:
- main

pool:
  name: self-hosted-agent
  
variables:
  resourceGroup: 'rg-nyc-taxi'
  dataFactoryName: 'adf-nyc-taxi-25'
  pipelineName: 'CopyNYCTaxiData'
  databricksWorkspaceURL: 'https://adb-2157200129015324.4.azuredatabricks.net'
  storageAccountName: 'nyctaxistorage2pvicw'
  synapseSQLEndpoint: 'synapse-nyctaxi-jbbhna.sql.azuresynapse.net'

stages:
- stage: RunADFPipeline
  displayName: 'ðŸš€ Trigger Data Factory Pipeline'
  jobs:
  - job: ExecuteADFCopy
    steps:
    - task: AzureCLI@2
      displayName: 'Trigger & Monitor ADF Pipeline'
      inputs:
        azureSubscription: 'AzureServiceConnection'
        scriptType: 'bash'
        scriptLocation: inlineScript
        inlineScript: | 
          # Trigger pipeline
          runId=$(az datafactory pipeline create-run \
            --factory-name "$(dataFactoryName)" \
            --resource-group "$(resourceGroup)" \
            --name "$(pipelineName)" \
            --query 'runId' -o tsv)
          
          echo "##vso[task.setvariable variable=ADF_RUN_ID]$runId"
          echo "ADF Pipeline Run ID: $runId"
          
          # Monitor pipeline
          while true; do
            status=$(az datafactory pipeline-run show \
              --factory-name "$(dataFactoryName)" \
              --resource-group "$(resourceGroup)" \
              --run-id "$runId" \
              --query 'status' -o tsv)
            
            echo "ADF Status: $status"
            
            if [[ "$status" == "Succeeded" ]]; then break
            elif [[ "$status" == "Failed" ]]; then exit 1
            fi
            sleep 30
          done

    - script: |
        echo "$ADF_RUN_ID" > adf_run_id.txt
      displayName: "Save ADF Run ID"
    
    - publish: adf_run_id.txt
      artifact: adf-run-id

- stage: ProcessInDatabricks
  displayName: 'âš¡ Process Data in Databricks'
  dependsOn: RunADFPipeline
  variables:
    - group: databricks-secrets
  jobs:
  - job: RunDatabricksNotebook
    steps:
    - checkout: self
    - download: current
      artifact: adf-run-id
    - script: |
        set -e  # Exit immediately on error
        set -x  # Enable debug logging

        # Verify working directory structure
        echo "##[debug]Current directory: $(pwd)"
        echo "##[debug]Directory contents:"
        ls -al

        # Get ADF Run ID
        runId=$(cat $(Pipeline.Workspace)/adf-run-id/adf_run_id.txt)
        echo "Using ADF Run ID: $runId"
        
        # Install dependencies with explicit path
        sudo apt update && sudo apt install -y python3-pip
        # Upgrade dependencies and configure CLI
        sudo python3 -m pip install --upgrade "urllib3>=1.26.0" chardet databricks-cli jq
        databricks jobs configure --version=2.1
        export PATH="/root/.local/bin:$PATH"
        
        # Verify notebook file exists
        NOTEBOOK_PATH="$(System.DefaultWorkingDirectory)/process_data.py"
        echo "##[debug]Notebook path: $NOTEBOOK_PATH"
        if [ ! -f "$NOTEBOOK_PATH" ]; then
          echo "##[error]Notebook file not found at $NOTEBOOK_PATH"
          exit 1
        fi
        
        # Upload Python script as notebook
        echo "##[section]Uploading notebook to Databricks..."
        databricks workspace import --format SOURCE --language PYTHON --overwrite "$NOTEBOOK_PATH" "/Shared/process_data"
        
        # Create and run job
        echo "##[section]Submitting Databricks job..."
        # Updated jobJson section using existing cluster
        jobJson=$(cat <<EOF
        {
          "name": "NYC Taxi Data Cleaning",
          "existing_cluster_id": "$(clusterId)",
          "notebook_task": {
            "notebook_path": "/Shared/process_data",
            "base_parameters": {
              "storage_account": "$(storageAccountName)",
              "raw_container": "nyc-taxi-raw",
              "synapse_endpoint": "$(synapseSQLEndpoint)",
              "synapse_database": "nyctaxipool"
            }
          }
        }
        EOF
        )
        
        # Submit job with error handling
        jobId=$(databricks jobs create --json "$jobJson" | jq -r .job_id) || {
            echo "##[error]Failed to create job";
            exit 1;
        }

        runId=$(databricks jobs run-now --job-id $jobId | jq -r .run_id) || {
            echo "##[error]Failed to start job";
            exit 1;
        }
        
        # Monitor job execution with proper timeout
        echo "##[section]Monitoring job execution..."
        timeout=1200  # 60 minutes
        start_time=$(date +%s)
        while :; do
          current_time=$(date +%s)
          elapsed=$((current_time - start_time))
          
          status=$(databricks runs get --run-id $runId | jq -r '.state.life_cycle_state')
          result=$(databricks runs get --run-id $runId | jq -r '.state.result_state')
          
          echo "Databricks Status: $status | Result: $result | Elapsed: ${elapsed}s"
          
          if [[ "$status" == "TERMINATED" ]]; then
            if [[ "$result" != "SUCCESS" ]]; then exit 1; fi
            break
          fi
          
          if (( elapsed > timeout )); then
            echo "##[error]Timeout after 60 minutes"
            exit 1
          fi
          sleep 30
        done
        
        echo "##[section]Cleaning up notebook..."
        databricks workspace delete /Shared/process_data
      displayName: 'Run Cleaning Notebook'
      env:
        dbToken: $(dbToken)
        databricksWorkspaceURL: $(databricksWorkspaceURL)
        DATABRICKS_HOST: $(databricksWorkspaceURL)
        DATABRICKS_TOKEN: $(dbToken)